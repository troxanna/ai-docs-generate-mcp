#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pdf_redact.py ‚Äî –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –ü–î–Ω –≤ PDF —Å –ø–æ–º–æ—â—å—é DeepPavlov ner_rus_bert + regex.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ YAML-—Ñ–∞–π–ª–∞ —Å–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏/—à–∞–±–ª–æ–Ω–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï–õ–¨–ó–Ø –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å.
  –ö–ª—é—á–∏ YAML:
    exact:  # —Å–ø–∏—Å–æ–∫ —Ç–æ—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (–±–µ–∑ —É—á—ë—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤)
      - "–û–û–û –†–æ–º–∞—à–∫–∞"
      - "–ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤"
    regex:  # —Å–ø–∏—Å–æ–∫ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π (Python/regex), —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–µ–∑ —Ñ–ª–∞–≥–æ–≤
      - "\\bTest\\d+\\b"
    by_label:  # –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫ (EMAIL/NAME/PHONE_RU/–∏ —Ç.–¥.)
      EMAIL:
        regex:
          - "@example\\.com$"
      NAME:
        exact:
          - "–ü–µ—Ç—Ä –ü–µ—Ç—Ä–æ–≤"

–ü—Ä–∏–º–µ—Ä—ã:
    python pdf_redact.py in.pdf out.pdf --log log.json \
        --enable-regex --enable-ner \
        --stop-yaml stoplist.yaml
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

# --- regex —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Æ–Ω–∏–∫–æ–¥–∞, fallback –Ω–∞ re ---
try:
    import regex as regx
except Exception:  # pragma: no cover
    import re as regx  # type: ignore

# ---------------------- –£—Ç–∏–ª–∏—Ç—ã ----------------------

def _norm_str(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: —Ç—Ä–∏–º, —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤, lower."""
    return " ".join((s or "").split()).strip().lower()

# ---------------------- –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-–ª–∏—Å—Ç–∞ ----------------------
class StopList:
    def __init__(self):
        self.global_exact: Set[str] = set()
        self.global_regex: List[regx.Pattern] = []
        # –ø–æ-–º–µ—Ç–æ—á–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º: label -> {"exact": set[str], "regex": list[Pattern]}
        self.by_label: Dict[str, Dict[str, object]] = {}

    @staticmethod
    def _compile_many(rx_list: List[str]) -> List[regx.Pattern]:
        out = []
        for pat in rx_list or []:
            try:
                out.append(regx.compile(pat))
            except Exception as e:
                print(f"[WARN] –ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π regex –∏–∑ YAML: {pat!r}: {e}", file=sys.stderr)
        return out

    @classmethod
    def from_yaml(cls, path: Path) -> "StopList":
        import yaml  # type: ignore
        sl = cls()
        try:
            data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
        except Exception as e:
            print(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å YAML —Å—Ç–æ–ø-—Å–ª–æ–≤: {e}", file=sys.stderr)
            sys.exit(4)

        # global exact
        for val in (data.get("exact") or []):
            if isinstance(val, str) and val.strip():
                sl.global_exact.add(_norm_str(val))

        # global regex
        sl.global_regex = cls._compile_many(list(data.get("regex") or []))

        # by_label
        by_label = data.get("by_label") or {}
        if isinstance(by_label, dict):
            for label, section in by_label.items():
                if not isinstance(section, dict):
                    continue
                exact_set: Set[str] = set()
                for val in (section.get("exact") or []):
                    if isinstance(val, str) and val.strip():
                        exact_set.add(_norm_str(val))
                regex_list = cls._compile_many(list(section.get("regex") or []))
                sl.by_label[str(label).upper()] = {"exact": exact_set, "regex": regex_list}
        return sl

    def blocks(self, token: str, label: Optional[str]) -> bool:
        tnorm = _norm_str(token)
        if not tnorm:
            return False
        # –≥–ª–æ–±–∞–ª—å–Ω—ã–µ exact/regex
        if tnorm in self.global_exact:
            return True
        for pr in self.global_regex:
            try:
                if pr.search(token):
                    return True
            except Exception:
                continue
        # –ø–æ-–º–µ—Ç–∫–µ
        if label:
            rules = self.by_label.get(label.upper())
            if rules:
                if tnorm in rules.get("exact", set()):
                    return True
                for pr in rules.get("regex", []):
                    try:
                        if pr.search(token):
                            return True
                    except Exception:
                        continue
        return False

# ---------------------- –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ü–î–Ω (RU) ----------------------

def build_ru_patterns():
    patterns = [
        ("EMAIL",       r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}"),
        ("PHONE_RU",    r"(?:(?:\+7|8)\s?(?:\(\d{3}\)|\d{3})[\s-]?)\d{3}[\s-]?\d{2}[\s-]?\d{2}"),
        ("INN",         r"\b(?:\d{10}|\d{12})\b"),
        ("SNILS",       r"\b\d{3}-\d{3}-\d{3}\s?\d{2}\b"),
        ("PASSPORT_RF", r"\b\d{2}\s?\d{2}\s?\d{6}\b"),
        ("CARD",        r"\b(?:\d{4}[-\s]?){3}\d{4}\b"),
        ("OGRN",        r"\b\d{13}\b"),
        ("OGRNIP",      r"\b\d{15}\b"),
        ("BIK",         r"\b\d{9}\b"),
        ("IBAN",        r"\b[A-Z]{2}\d{2}[A-Z0-9]{10,30}\b"),
        ("SWIFT",       r"\b[A-Z]{4}[A-Z]{2}[A-Z0-9]{2}(?:[A-Z0-9]{3})?\b"),
    ]
    return [(name, regx.compile(rx)) for name, rx in patterns]

# ---------------------- –§–ò–û-—à–∞–±–ª–æ–Ω—ã ----------------------

def fio_matchers():
    cyr_particle = r"(?:(?:[–¥–î]–µ|[—Ñ–§]–æ–Ω)\s+)?"
    lat_particle = r"(?:(?i:(?:o'|d'|de|van|von|da|di|du|del|della|de\s+la)\s+))?"
    cyr_name = r"[–ê-–Ø–Å][–∞-—è—ë]+(?:-[–ê-–Ø–Å][–∞-—è—ë]+)?"
    lat_name = r"[A-Z][a-z]+(?:-[A-Z][a-z]+)?"

    fio3_cyr = regx.compile(r"\b" + cyr_particle + cyr_name + r"\s+" + cyr_name + r"\s+" + cyr_name + r"\b")
    fio2_cyr = regx.compile(r"\b" + cyr_particle + cyr_name + r"\s+" + cyr_name + r"\b")
    init2_cyr = regx.compile(r"\b" + cyr_particle + cyr_name + r"\s+[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\b")
    init1_cyr = regx.compile(r"\b" + cyr_particle + cyr_name + r"\s+[–ê-–Ø–Å]\.\b")

    fio3_lat = regx.compile(r"\b" + lat_particle + lat_name + r"\s+" + lat_name + r"\s+" + lat_name + r"\b", regx.I)
    fio2_lat = regx.compile(r"\b" + lat_particle + lat_name + r"\s+" + lat_name + r"\b", regx.I)
    init2_lat = regx.compile(r"\b" + lat_particle + lat_name + r"\s+[A-Z]\.\s*[A-Z]\.\b", regx.I)
    init1_lat = regx.compile(r"\b" + lat_particle + lat_name + r"\s+[A-Z]\.\b", regx.I)

    return dict(
        fio3_cyr=fio3_cyr, fio2_cyr=fio2_cyr, init2_cyr=init2_cyr, init1_cyr=init1_cyr,
        fio3_lat=fio3_lat, fio2_lat=fio2_lat, init2_lat=init2_lat, init1_lat=init1_lat
    )

# ---------------------- –°–±–æ—Ä PERSON –∏–∑ BIO-—Ç–µ–≥–æ–≤ ----------------------

def collect_person_spans(tokens, tags):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ PERSON –∏–∑ BIO-—Ç–µ–≥–æ–≤ (B-PER / I-PER / O).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (–±–µ–∑ –¥–æ–ø. –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏).
    """
    spans = []
    cur = []
    for tok, tag in zip(tokens, tags):
        tag = tag.upper()
        if tag.endswith("PER"):
            if tag.startswith("B-") or (cur and tag.startswith("B-")):
                if cur:
                    spans.append(" ".join(cur))
                    cur = []
                cur = [tok]
            elif tag.startswith("I-"):
                if not cur:
                    cur = [tok]
                else:
                    cur.append(tok)
            else:
                # –ø—Ä–æ—Å—Ç–æ "PER" –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ ‚Äî –Ω–∞ –≤—Å—è–∫–∏–π
                cur.append(tok)
        else:
            if cur:
                spans.append(" ".join(cur))
                cur = []
    if cur:
        spans.append(" ".join(cur))
    # –º–∏–Ω–∏-—á–∏—Å—Ç–∫–∞ –¥–≤–æ–π–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    return [" ".join(s.split()) for s in spans if s.strip()]

# ---------------------- NER —á–µ—Ä–µ–∑ DeepPavlov ----------------------

def build_ner():
    from deeppavlov import configs, build_model
    # –∑–∞–≥—Ä—É–∑–∏—Ç/—Å–∫–∞—á–∞–µ—Ç –º–æ–¥–µ–ª—å –ø–æ –∫–æ–Ω—Ñ–∏–≥—É ner_rus_bert
    ner_model = build_model(configs.ner.ner_rus_bert, download=True)
    return ner_model

# ---------------------- –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç ----------------------

def main():
    ap = argparse.ArgumentParser(description="Redact PII in PDF using DeepPavlov ner_rus_bert + regex (RU).")
    ap.add_argument("input", help="–í—Ö–æ–¥–Ω–æ–π PDF")
    ap.add_argument("output", help="–í—ã—Ö–æ–¥–Ω–æ–π (—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π) PDF")
    ap.add_argument("--log", required=True, help="–ü—É—Ç—å –∫ JSON-–ª–æ–≥—É")
    ap.add_argument("--enable-ner", action="store_true", help="–í–∫–ª—é—á–∏—Ç—å DeepPavlov NER (PERSON)")
    ap.add_argument("--enable-regex", action="store_true", default=True, help="–í–∫–ª—é—á–∏—Ç—å regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –ü–î–Ω (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª.)")
    # —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    ap.add_argument("--images-mode", type=int, default=2, help="images —Ä–µ–∂–∏–º –≤ apply_redactions (0..3), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2")
    ap.add_argument("--graphics-mode", type=int, default=2, help="graphics —Ä–µ–∂–∏–º (0..2), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2")
    ap.add_argument("--text-mode", type=int, default=0, help="text —Ä–µ–∂–∏–º (0..1), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0 (—É–¥–∞–ª—è—Ç—å)")
    ap.add_argument("--min-font", type=float, default=5.0, help="–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤—ã—Å–æ—Ç–∞ bbox –¥–ª—è —É—á—ë—Ç–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è")
    ap.add_argument("--show-token", action="store_true", help="–ø–µ—á–∞—Ç—å –∑–∞–º–µ–Ω–∏—Ç–µ–ª—è –ø–æ–≤–µ—Ä—Ö (–∏–Ω–∞—á–µ —Å–ø–ª–æ—à–Ω–∞—è –∑–∞–ª–∏–≤–∫–∞)")
    ap.add_argument("--replacement", default="", help="—Ç–µ–∫—Å—Ç –∑–∞–º–µ–Ω–∏—Ç–µ–ª—è –ø—Ä–∏ --show-token")
    ap.add_argument("--fio-allow-two", action="store_true", default=True, help="–§–∞–º–∏–ª–∏—è –ò–º—è")
    ap.add_argument("--fio-allow-initials", action="store_true", default=True, help="–§–∞–º–∏–ª–∏—è –ò.–û./–§–∞–º–∏–ª–∏—è –ò.")
    ap.add_argument("--fio-allow-latin", action="store_true", default=True, help="–ª–∞—Ç–∏–Ω–∏—Ü–∞ (Ivanov Ivan / Ivanov I.)")
    # –ù–æ–≤–æ–µ: —Å—Ç–æ–ø-–ª–∏—Å—Ç –∏–∑ YAML
    ap.add_argument("--stop-yaml", dest="stop_yaml", help="–ü—É—Ç—å –∫ YAML —Å–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏/—à–∞–±–ª–æ–Ω–∞–º–∏ (–∑–∞–ø—Ä–µ—â–µ–Ω–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å)")

    args = ap.parse_args()

    in_path = Path(args.input)
    out_path = Path(args.output)
    log_path = Path(args.log)

    if not in_path.exists():
        print(f"[ERROR] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {in_path}", file=sys.stderr)
        sys.exit(2)

    # –ò–º–ø–æ—Ä—Ç PyMuPDF
    try:
        import fitz  # PyMuPDF
    except Exception as e:
        print("[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PyMuPDF (–ø–∞–∫–µ—Ç 'pymupdf').", file=sys.stderr)
        sys.exit(3)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    matcher = fio_matchers()
    compiled_patterns = build_ru_patterns() if args.enable_regex else []
    ner = None
    if args.enable_ner:
        try:
            ner = build_ner()
        except Exception as e:
            print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å DeepPavlov NER: {e}", file=sys.stderr)

    stoplist = None
    if args.stop_yaml:
        try:
            stoplist = StopList.from_yaml(Path(args.stop_yaml))
        except ModuleNotFoundError:
            print("[ERROR] –î–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ --stop-yaml —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–∞–∫–µ—Ç PyYAML (pip install pyyaml)", file=sys.stderr)
            sys.exit(5)

    all_logs = []

    with fitz.open(str(in_path)) as doc:
        for page_index, page in enumerate(doc):
            page_text = page.get_text("text") or ""
            found_items: List[Tuple[str, str, str]] = []  # (text, label, source)

            # ---- REGEX –ü–î–Ω ----
            if compiled_patterns and page_text.strip():
                for name, patt in compiled_patterns:
                    for m in patt.finditer(page_text):
                        val = m.group(0).strip()
                        if val:
                            found_items.append((val, name, "regex"))

            # ---- NER (PERSON) —á–µ—Ä–µ–∑ DeepPavlov ----
            if ner and page_text.strip():
                # –í DeepPavlov —É–¥–æ–±–Ω–µ–µ –ø–æ–¥–∞–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
                lines = [ln for ln in page_text.splitlines() if ln.strip()]
                for ln in lines:
                    try:
                        tokens, tags = ner([ln])
                        if not tokens or not tags:
                            continue
                        tokens = tokens[0]
                        tags = tags[0]
                        spans = collect_person_spans(tokens, tags)  # —Å—Ç—Ä–æ–∫–∏ PERSON
                        for s in spans:
                            exacts = []
                            for mm in matcher["fio3_cyr"].finditer(s): exacts.append(mm.group(0))
                            for mm in matcher["fio3_lat"].finditer(s): exacts.append(mm.group(0))
                            if args.fio_allow_two:
                                for mm in matcher["fio2_cyr"].finditer(s): exacts.append(mm.group(0))
                                for mm in matcher["fio2_lat"].finditer(s): exacts.append(mm.group(0))
                            if args.fio_allow_initials:
                                for mm in matcher["init2_cyr"].finditer(s): exacts.append(mm.group(0))
                                for mm in matcher["init1_cyr"].finditer(s): exacts.append(mm.group(0))
                                for mm in matcher["init2_lat"].finditer(s): exacts.append(mm.group(0))
                                for mm in matcher["init1_lat"].finditer(s): exacts.append(mm.group(0))
                            if exacts:
                                for ex in exacts:
                                    found_items.append((ex, "NAME", "ner"))
                            else:
                                found_items.append((" ".join(s.split()), "NAME", "ner"))
                    except Exception:
                        continue

            # ---- –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–æ–ø-–ª–∏—Å—Ç: —Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã ----
            if stoplist:
                filtered_items: List[Tuple[str, str, str]] = []
                skipped = 0
                for token, label, source in found_items:
                    if stoplist.blocks(token, label):
                        skipped += 1
                        continue
                    filtered_items.append((token, label, source))
                found_items = filtered_items
            else:
                skipped = 0

            # ---- –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º ----
            page_log = []
            seen_rects = set()

            for token, label, source in found_items:
                # 1) –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫
                rects = page.search_for(token)
                # 2) ¬´–º—è–≥–∫–∏–π¬ª –ø–æ–∏—Å–∫
                if not rects:
                    rects = page.search_for(token, hit_max=100, quads=False)
                # 3) –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–±–µ–ª–∞–º
                if not rects:
                    token_norm = " ".join(token.split())
                    if token_norm and token_norm != token:
                        rects = page.search_for(token_norm) or page.search_for(token_norm, hit_max=100, quads=False)

                for r in rects or []:
                    if (r.y1 - r.y0) < args.min_font:
                        continue
                    sig = (round(r.x0, 2), round(r.y0, 2), round(r.x1, 2), round(r.y1, 2), token)
                    if sig in seen_rects:
                        continue
                    seen_rects.add(sig)

                    if args.show_token:
                        page.add_redact_annot(r, text=(args.replacement or token), fill=(0, 0, 0))
                    else:
                        page.add_redact_annot(r, fill=(0, 0, 0))

                    page_log.append({
                        "page": page_index + 1,
                        "label": label,
                        "source": source,
                        "text": token,
                        "rect": [r.x0, r.y0, r.x1, r.y1],
                    })

            if page_log:
                page.apply_redactions(images=args.images_mode, graphics=args.graphics_mode, text=args.text_mode)

            # –¥–æ–±–∞–≤–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–ø—É—Å–∫–∞—Ö –ø–æ —Å—Ç–æ–ø-–ª–∏—Å—Ç—É
            if skipped:
                page_log.append({
                    "page": page_index + 1,
                    "skipped_by_stoplist": int(skipped)
                })

            all_logs.extend(page_log)

        doc.save(str(out_path), garbage=4, deflate=True, clean=True, incremental=False)

    # ---- –õ–û–ì ----
    payload = {
        "input": str(in_path),
        "output": str(out_path),
        "ner": bool(ner is not None),
        "regex_enabled": bool(compiled_patterns),
        "stop_yaml": str(args.stop_yaml or ""),
        "redactions": all_logs,
    }
    Path(log_path).write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"‚úî –ì–æ—Ç–æ–≤–æ: {out_path}\nüìù –õ–æ–≥: {log_path}\n–†–µ–¥–∞–∫—Ü–∏–π: {len([x for x in all_logs if 'rect' in x])}")


if __name__ == "__main__":
    main()
